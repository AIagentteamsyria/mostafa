# -*- coding: utf-8 -*-
# Ø§Ù„Ù…Ù„Ù: analyze_page.py

import pandas as pd
from playwright.sync_api import sync_playwright, Error as PlaywrightError
from bs4 import BeautifulSoup, element as bs4_element
import json
import time
import sys # Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±

# (ÙƒÙ„ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆØ¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ ØªØºÙŠÙŠØ±)

def generate_control_selector(element: bs4_element.Tag) -> str:
    if element.has_attr('id'): return f"#{element['id']}"
    if element.has_attr('class'):
        classes = [c for c in element['class'] if c]
        if classes: return f"{element.name}.{'.'.join(classes)}"
    return element.name

def write_content_json_file(filename: str, media: list, interactives: list, texts: list):
    page_content_structure = {
        "media_content": {"count": len(media), "items": media},
        "interactive_elements": {"count": len(interactives), "items": interactives},
        "main_texts": {"count": len(texts), "items": texts}
    }
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(page_content_structure, f, ensure_ascii=False, indent=4)

def analyze_live_chrome_page(target_url: str, csv_filename: str = "live_profile_analysis.csv", 
                             json_filename: str = "live_profile_content.json"):
    print("ğŸš€ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù…ØªØµÙØ­ ÙƒØ±ÙˆÙ… Ø§Ù„Ø°ÙŠ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° 9222...")
    with sync_playwright() as p:
        try:
            browser = p.chromium.connect_over_cdp("http://localhost:9222")
            context = browser.contexts[0]
            print(f"Ø¬Ø§Ø±ÙŠ ÙØªØ­ ØªØ¨ÙˆÙŠØ¨ Ø¬Ø¯ÙŠØ¯ ÙˆØ§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰: {target_url} ...")
            page = context.new_page()
            page.goto(target_url, wait_until='networkidle', timeout=90000)
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­.")
            
            print("\n" + "="*50)
            print("--- ğŸ”´ Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) ---")
            print("1. Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø¢Ù† Ø£Ù…Ø§Ù…Ùƒ ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­. Ù‚Ù… Ø¨Ø£ÙŠ ØªØ¬Ù‡ÙŠØ²Ø§Øª Ù†Ù‡Ø§Ø¦ÙŠØ©.")
            input("\nâ¬…ï¸  Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ù‹Ø§ØŒ Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Enter Ù‡Ù†Ø§ Ù„Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„...")
            
            print(f"\nâœ… ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø£Ù…Ø±! Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {page.title()}")
            time.sleep(2)
            
            page_content = page.content()
            soup = BeautifulSoup(page_content, 'lxml')
            all_tags = soup.find_all(True)
            print(f"ğŸ”¬ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(all_tags)} Ø¹Ù†ØµØ±ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
            
            all_elements_data_for_csv, media_elements, interactive_elements, text_elements = [], [], [], []
            MEDIA_TAGS, INTERACTIVE_TAGS, TEXT_TAGS = {'img', 'video', 'audio', 'source', 'picture'}, {'a', 'button', 'input', 'select', 'textarea'}, {'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote', 'span'}
            
            for i, element in enumerate(all_tags, 1):
                attributes, control_selector, text_content = element.attrs, generate_control_selector(element), element.get_text(separator=' ', strip=True)
                all_elements_data_for_csv.append({'element_id': i, 'tag_name': element.name, 'control_selector': control_selector, 'text_content': text_content or None, 'id_attr': attributes.get('id'), 'class_attr': ' '.join(attributes.get('class', [])), 'href_attr': attributes.get('href'), 'src_attr': attributes.get('src'), 'all_attributes_json': json.dumps(attributes, ensure_ascii=False)})
                tag_name = element.name.lower()
                if tag_name in MEDIA_TAGS and element.has_attr('src'): media_elements.append({'tag': tag_name, 'src': element['src']})
                elif tag_name in INTERACTIVE_TAGS or element.has_attr('onclick'): interactive_elements.append({'tag': tag_name, 'text': text_content, 'selector': control_selector})
                elif tag_name in TEXT_TAGS and text_content and len(text_content) > 15:
                    if not element.find_parent(lambda p: p.name in INTERACTIVE_TAGS): text_elements.append(text_content)
            
            df = pd.DataFrame(all_elements_data_for_csv)
            df.rename(columns={'control_selector': 'ØªØ§Øº Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ø¹Ù†ØµØ±'}, inplace=True)
            df = df[['element_id', 'tag_name', 'ØªØ§Øº Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ø¹Ù†ØµØ±', 'text_content', 'id_attr', 'class_attr', 'href_attr', 'src_attr', 'all_attributes_json']]
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"\nğŸ‰ Ù†Ø¬Ø§Ø­! ØªÙ… Ø­ÙØ¸ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØµÙ„ ÙÙŠ: {csv_filename}")
            
            write_content_json_file(json_filename, media_elements, interactive_elements, list(set(text_elements)))
            print(f"ğŸ’¾ Ù†Ø¬Ø§Ø­! ØªÙ… Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª ÙÙŠ Ù…Ù„Ù JSON: {json_filename}")

        except PlaywrightError as e:
            print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ù…Ù† Playwright: {e}")
            print("  - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ùƒ Ù‚Ù…Øª Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ØªØµÙØ­ Ø£ÙˆÙ„Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `launch_browser.py` ÙˆØ£Ù† Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…ØªØµÙØ­ Ù…Ø§ Ø²Ø§Ù„Øª Ù…ÙØªÙˆØ­Ø©.")
        except Exception as e:
            print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙˆØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        finally:
            print("\nğŸ‘ ØªÙ… Ù‚Ø·Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ØªØµÙØ­ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø©.")
            if 'browser' in locals() and browser.is_connected():
                browser.close()

# --- Ù†Ù‚Ø·Ø© Ø§Ù†Ø·Ù„Ø§Ù‚ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ (ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§) ---
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ± Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø©.")
        print("ğŸ’¡ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµØ­ÙŠØ­Ø©:")
        print(f"   python {sys.argv[0]} <Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ØµÙØ­Ø©>")
        print("\nÙ…Ø«Ø§Ù„:")
        print(f"   python {sys.argv[0]} https://myaccount.google.com")
        sys.exit(1)

    target_url = sys.argv[1]
    if not (target_url.startswith("http://") or target_url.startswith("https://")):
        print("Ø§Ù„Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http:// Ø£Ùˆ https://.")
    else:
        analyze_live_chrome_page(target_url=target_url)